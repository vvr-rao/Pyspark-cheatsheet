{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52c2be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9a7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession\\\n",
    "    .builder\\\n",
    "    .appName(\"SparkETL\")\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dfbdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.csv(\"s3://BUCKET_NAME/input/2017_StPaul_MN_Real_Estate.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5276f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d584c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our dependent variable\n",
    "Y_df = df.select(['SalesClosePrice'])\n",
    "\n",
    "# Display summary statistics\n",
    "Y_df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42c9bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8dc99a3",
   "metadata": {},
   "source": [
    "## FInd Correlations##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad19993",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericcols = [item[0] for item in df.dtypes if (item[1].startswith('int') or item[1].startswith('double'))]\n",
    "print(numericcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "766238b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericcols.remove('No.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5062f245",
   "metadata": {},
   "outputs": [],
   "source": [
    "numericcols.remove('SalesClosePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38b32733",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(numericcols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f681463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop to check all columns contained in list\n",
    "for col in numericcols:\n",
    "    #print(col)\n",
    "    #print(df.corr('SalesClosePrice', col))\n",
    "    print(\"Column: {} Corr: {} \".format(col, df.corr('SalesClosePrice', col)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a5f7e",
   "metadata": {},
   "source": [
    "## Get Skewness ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80cdd9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import skewness function\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "# Loop to check all columns contained in list\n",
    "for col in numericcols:\n",
    "    print(\"Column: {} Skewness: {} \".format(col, df.agg({col: 'skewness'}).collect()))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c262b47",
   "metadata": {},
   "source": [
    "## Remove Outliers ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42062a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1e5d13",
   "metadata": {},
   "outputs": [],
   "source": [
    "std_dev = df.agg({'SALESCLOSEPRICE': 'stddev'}).collect()[0][0]\n",
    "mean_val = df.agg({'SALESCLOSEPRICE': 'mean'}).collect()[0][0]\n",
    "\n",
    "h_bound = mean_val + (3 * std_dev)\n",
    "l_bound = mean_val - (3 * std_dev)\n",
    "\n",
    "df_withoutoutliers = df.where((df['SALESCLOSEPRICE'] < h_bound) & (df['SALESCLOSEPRICE'] > l_bound))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1426d865",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_withoutoutliers.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200ac3b8",
   "metadata": {},
   "source": [
    "## Scaling ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9dc94e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(df, cols_to_scale):\n",
    "  # Takes a dataframe and list of columns to minmax scale. Returns a dataframe.\n",
    "  for col in cols_to_scale:\n",
    "    # Define min and max values and collect them\n",
    "    max_days = df.agg({col: 'max'}).collect()[0][0]\n",
    "    min_days = df.agg({col: 'min'}).collect()[0][0]\n",
    "    new_column_name = 'scaled_' + col\n",
    "    # Create a new column based off the scaled data\n",
    "    df = df.withColumn(new_column_name, \n",
    "                      (df[col] - min_days) / (max_days - min_days))\n",
    "  return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7753f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols_to_scale = ['DAYSONMARKET']\n",
    "df_scaled = min_max_scaler(df_withoutoutliers, cols_to_scale)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06247367",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d92a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.select(['DAYSONMARKET']).describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2989e067",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.select(['scaled_DAYSONMARKET']).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825a5388",
   "metadata": {},
   "source": [
    "## IMPUTATION OF MISSING VALUES ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d926da09",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_mean = df_scaled.agg({'scaled_DAYSONMARKET': 'mean'}).collect()[0][0]\n",
    "df_scaled = df_scaled.fillna(col_mean, subset=['scaled_DAYSONMARKET'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a1c843",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_scaled.select(['scaled_DAYSONMARKET']).describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a9f44",
   "metadata": {},
   "source": [
    "## LEFT SKEW ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d417fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg({'SALESCLOSEPRICE': 'skewness'}).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbf2f1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log\n",
    "df = df.withColumn(\"log_SalesClosePrice\", log(df['SALESCLOSEPRICE']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1387a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.agg({'log_SalesClosePrice': 'skewness'}).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0161c58d",
   "metadata": {},
   "source": [
    "## RIGHT SKEW ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136eeef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the skewness\n",
    "print(df.agg({'YEARBUILT': 'skewness'}).collect())\n",
    "\n",
    "# Calculate the max year\n",
    "max_year = df.agg({'YEARBUILT': 'max'}).collect()[0][0]\n",
    "\n",
    "# Create a new column of reflected data\n",
    "df = df.withColumn('Reflect_YearBuilt', (max_year + 1) - df['YEARBUILT'])\n",
    "\n",
    "# Create a new column based reflected data\n",
    "df = df.withColumn('adj_yearbuilt', 1 / log(df['Reflect_YearBuilt']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7921cd87",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.agg({'adj_yearbuilt': 'skewness'}).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cc3732",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
